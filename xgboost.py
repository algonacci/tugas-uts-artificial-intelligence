# -*- coding: utf-8 -*-
"""XGBoost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uEb6oreEiLxbE2wVT9XC-Eb3cM-P9N2f
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# --- Library XGBoost untuk Model ---
# XGBoost (eXtreme Gradient Boosting): Algoritma machine learning yang sangat efisien dan efektif, sering menjadi pemenang di kompetisi data.
import xgboost as xgb

# --- Library Google Colab ---
# files: Modul untuk mengunggah file dari komputer lokal ke sesi Colab.
from google.colab import files

# --- Pengaturan Visualisasi ---
# Mengatur gaya default dari Seaborn agar plot lebih menarik dan mudah dibaca.
sns.set_style('whitegrid')
# Mengatur ukuran default plot
plt.rcParams['figure.figsize'] = (12, 8)

print("Semua library berhasil diimpor dan lingkungan siap digunakan.")

# Menampilkan dialog untuk mengunggah file
print("Silakan pilih file 'kdd_train.csv' dan 'kdd_test.csv' dari komputer Anda.")
uploaded = files.upload()

# Memuat file yang telah diunggah ke dalam DataFrame Pandas
try:
    train_df = pd.read_csv('kdd_train.csv')
    test_df = pd.read_csv('kdd_test.csv')
    print("\nDataset berhasil dimuat ke dalam DataFrame!")
    print("-" * 50)
    print(f"Data training memiliki {train_df.shape[0]} baris dan {train_df.shape[1]} kolom.")
    print(f"Data testing memiliki {test_df.shape[0]} baris dan {test_df.shape[1]} kolom.")
    print("-" * 50)
except FileNotFoundError:
    print("\nError: File tidak ditemukan. Pastikan nama file benar (kdd_train.csv, kdd_test.csv) dan sudah diunggah dengan benar.")

# Menampilkan 5 baris pertama data untuk melihat struktur dan contoh nilainya
print("--- 5 Baris Pertama Data Training ---")
display(train_df.head())

# Menampilkan informasi detail tentang DataFrame, termasuk tipe data dan jumlah nilai non-null
# Ini membantu mengidentifikasi kolom kategorikal dan memeriksa missing values
print("\n--- Informasi Detail Data Training ---")
train_df.info()

# Menampilkan statistik deskriptif (mean, std, min, max, kuartil) untuk kolom numerik
# Ini memberikan gambaran tentang distribusi dan potensi outlier
print("\n--- Statistik Deskriptif Data Training ---")
display(train_df.describe())

# Menampilkan 5 baris pertama data untuk melihat struktur dan contoh nilainya
print("--- 5 Baris Pertama Data Training ---")
display(train_df.head())

# Menampilkan informasi detail tentang DataFrame, termasuk tipe data dan jumlah nilai non-null
# Ini membantu mengidentifikasi kolom kategorikal dan memeriksa missing values
print("\n--- Informasi Detail Data Training ---")
train_df.info()

# Menampilkan statistik deskriptif (mean, std, min, max, kuartil) untuk kolom numerik
# Ini memberikan gambaran tentang distribusi dan potensi outlier
print("\n--- Statistik Deskriptif Data Training ---")
display(train_df.describe())

# Menghitung jumlah kemunculan setiap label kelas
label_counts = train_df['labels'].value_counts()
print("--- Distribusi Kelas pada Kolom 'labels' (Data Training) ---")
print(label_counts)

# Visualisasi distribusi kelas untuk memahami ketidakseimbangan data
plt.figure(figsize=(18, 10))
sns.countplot(y='labels', data=train_df, order=label_counts.index)
plt.title('Distribusi Kelas Serangan di Data Training', fontsize=18)
plt.xlabel('Jumlah Sampel', fontsize=14)
plt.ylabel('Jenis Serangan (Label)', fontsize=14)
plt.show()

# Analisis: Terlihat jelas bahwa data sangat tidak seimbang. 'normal.' dan 'neptune.' mendominasi.
# Ini penting karena model bisa saja "malas" dan hanya memprediksi kelas mayoritas untuk mendapatkan akurasi tinggi,
# tetapi gagal mendeteksi serangan yang jarang terjadi.

# Mengidentifikasi dan menganalisis fitur-fitur kategorikal
categorical_features = train_df.select_dtypes(include=['object']).columns.tolist()
# Kecualikan kolom target
categorical_features.remove('labels')

print(f"\n--- Fitur Kategorikal: {categorical_features} ---")

for feature in categorical_features:
    print(f"\nJumlah nilai unik untuk '{feature}': {train_df[feature].nunique()}")
    # Tampilkan 10 nilai teratas untuk fitur dengan banyak kategori
    if train_df[feature].nunique() > 10:
        print("10 nilai teratas:")
        print(train_df[feature].value_counts().head(10))
    else:
        print(train_df[feature].value_counts())

# Analisis: Fitur 'service' memiliki sangat banyak kategori. Ini akan menghasilkan banyak kolom setelah encoding.
# 'protocol_type' dan 'flag' memiliki jumlah kategori yang lebih sedikit.

# Memisahkan fitur (X) dan target (y)
X_train = train_df.drop('labels', axis=1)
y_train = train_df['labels']
X_test = test_df.drop('labels', axis=1)
y_test = test_df['labels']

print("Memisahkan fitur dan target selesai.")

# --- Strategi Encoding Kategorikal yang Aman ---
# Masalah: Jika kita melakukan encoding secara terpisah, bisa jadi ada kategori di data test yang tidak ada di data train.
# Ini akan menyebabkan jumlah kolom berbeda dan menyebabkan error.

# Solusi: Gabungkan data training dan testing, lalu lakukan encoding. Ini memastikan semua kategori diakui.
print("\nMenggabungkan data training dan testing untuk konsistensi encoding...")
all_data = pd.concat([X_train, X_test], keys=['train', 'test'])

# Identifikasi ulang kolom kategorikal dari data gabungan
categorical_cols = all_data.select_dtypes(include=['object']).columns
print(f"Kolom kategorikal yang akan di-encode: {categorical_cols.tolist()}")

# Lakukan One-Hot Encoding. Ini mengubah setiap kategori menjadi kolom biner baru (0/1).
all_data_encoded = pd.get_dummies(all_data, columns=categorical_cols)

# Pisahkan kembali data training dan testing yang sudah di-encode berdasarkan kunci yang kita buat
X_train_encoded = all_data_encoded.xs('train')
X_test_encoded = all_data_encoded.xs('test')

print("\n--- Hasil Encoding Fitur ---")
print(f"Shape data training setelah encoding: {X_train_encoded.shape}")
print(f"Shape data testing setelah encoding: {X_test_encoded.shape}")
print("Encoding fitur kategorikal selesai. Kedua dataset sekarang memiliki struktur kolom yang identik.")

# --- Strategi Encoding Label yang Aman ---
# Masalah: LabelEncoder.transform() akan error jika menemukan label baru di data test yang tidak ada di data train.
# Model tidak bisa memprediksi sesuatu yang tidak pernah dipelajari.

# Solusi: Saring data testing untuk hanya menyertakan label yang sudah dikenal oleh model.
print("\nMemproses label kelas (target)...")

# 1. Inisialisasi dan latih LabelEncoder HANYA dengan data training
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)

# 2. Simpan daftar label yang dikenal oleh model
known_labels = label_encoder.classes_
print(f"Jumlah label yang dikenal model: {len(known_labels)}")

# 3. Saring data testing
print("Menyaring data testing untuk menghapus label yang tidak dikenal...")
# Buat mask boolean: True jika label di y_test ada di known_labels
mask = y_test.isin(known_labels)

# Terapkan mask untuk menyaring X_test dan y_test
X_test_filtered = X_test_encoded[mask]
y_test_filtered = y_test[mask]

print(f"Shape data testing awal: {X_test_encoded.shape}")
print(f"Shape data testing setelah disaring: {X_test_filtered.shape}")
print(f"Jumlah data testing yang dibuang (karena label tidak dikenal): {len(y_test) - len(y_test_filtered)}")

# 4. Encode label testing yang sudah disaring
y_test_encoded = label_encoder.transform(y_test_filtered)

print("\n--- Mapping Label (dari data training) ---")
for i, class_name in enumerate(label_encoder.classes_):
    print(f"{i}: {class_name}")

print("\nPra-pemrosesan label selesai.")

# --- Inisialisasi Model XGBoost ---
# Penjelasan parameter:
# objective='multi:softmax': Menentukan tugas sebagai klasifikasi multi-kelas. Softmax menghasilkan probabilitas untuk setiap kelas.
# num_class: Jumlah kelas unik. Harus disesuaikan dengan output dari LabelEncoder.
# use_label_encoder=False: Scikit-learn versi baru meminta ini untuk menghindari warning internal.
# eval_metric='mlogloss': Metrik evaluasi 'multi-class log-loss' yang baik untuk klasifikasi multi-kelas.
# random_state=42: Untuk reproduktifitas. Memastikan hasil yang sama setiap kali dijalankan.
# n_jobs=-1: Menggunakan semua core CPU yang tersedia untuk mempercepat proses pelatihan.
model = xgb.XGBClassifier(
    objective='multi:softmax',
    num_class=len(label_encoder.classes_),
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42,
    n_jobs=-1
)

# --- Pelatihan Model ---
# Proses .fit() adalah di mana model "belajar" dari data.
# XGBoost bekerja dengan membangun pohon keputusan secara berurutan.
# Setiap pohon baru berusaha memperbaiki kesalahan yang dibuat oleh pohon-pohon sebelumnya (boosting).
print("\n--- Memulai Pelatihan Model XGBoost ---")
model.fit(X_train_encoded, y_train_encoded)
print("Model XGBoost berhasil dilatih!")

# --- Membuat Prediksi ---
# Gunakan model yang sudah dilatih untuk memprediksi label pada data testing yang sudah disaring.
y_pred_encoded = model.predict(X_test_filtered)

# Kembalikan hasil prediksi (yang berupa angka) ke dalam label asli (teks) untuk kemudahan interpretasi.
y_pred = label_encoder.inverse_transform(y_pred_encoded)

# --- 7.1: Akurasi ---
# Akurasi adalah rasio prediksi yang benar terhadap total prediksi.
# Penting: Pada data tidak seimbang, akurasi bisa menyesatkan.
accuracy = accuracy_score(y_test_filtered, y_pred)
print(f"Akurasi Model: {accuracy:.4f}")

# --- 7.2: Laporan Klasifikasi ---
# Laporan ini memberikan metrik yang lebih detail untuk setiap kelas.
# Precision: Dari semua prediksi untuk kelas X, berapa banyak yang benar? (Presisi)
# Recall: Dari semua data asli kelas X, berapa banyak yang berhasil ditemukan model? (Sensitivitas)
# F1-Score: Rata-rata harmonis dari Precision dan Recall. Metrik yang seimbang.
print("\n--- Laporan Klasifikasi Detail ---")
print(classification_report(y_test_filtered, y_pred))
# Analisis: Perhatikan F1-score untuk kelas minoritas. Jika rendah, berarti model kesulitan mendeteksinya.

# Confusion Matrix menunjukkan performa model per kelas.
# Diagonal utama (kiri atas ke kanan bawah) menunjukkan prediksi yang benar.
# Nilai di luar diagonal menunjukkan kesalahan klasifikasi.
cm = confusion_matrix(y_test_filtered, y_pred)

plt.figure(figsize=(20, 18))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix', fontsize=20)
plt.ylabel('Label Asli (Actual)', fontsize=15)
plt.xlabel('Label Prediksi (Predicted)', fontsize=15)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.show()
# Analisis: Cari sel di luar diagonal yang bernilai tinggi. Ini menunjukkan di mana model paling sering bingung.

# XGBoost memiliki kemampuan bawaan untuk menghitung seberapa penting setiap fitur dalam membuat prediksi.
# Ini memberikan wawasan tentang karakteristik apa yang paling menentukan suatu koneksi adalah serangan.
importances = model.feature_importances_
features = X_train_encoded.columns

# Buat DataFrame untuk kemudahan plotting
feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})
# Urutkan dan ambil 20 fitur teratas
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).head(20)

# Visualisasi
plt.figure(figsize=(12, 10))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')
plt.title('20 Fitur Paling Penting Menurut Model XGBoost', fontsize=20)
plt.xlabel('Tingkat Kepentingan (Importance Score)', fontsize=15)
plt.ylabel('Fitur', fontsize=15)
plt.show()
# Analisis: Fitur seperti 'src_bytes', 'dst_bytes', 'count' adalah indikator kuat aktivitas jaringan.

# --- Install library yang diperlukan ---
!pip install imbalanced-learn -q

# Import SEMUA library yang diperlukan
from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split
from imblearn.over_sampling import SMOTE
from scipy.stats import uniform, randint
from collections import Counter
import time
import pandas as pd
import numpy as np

# ==============================================================================
# LANGKAH 8 (VERSI "ULTRA-FAST"): OPTIMASI MODEL KURANG DARI 5 MENIT
# ==============================================================================

# --- 8.1: Sampling dan Filtering Data (Versi Ultra-Fast) ---
print("\n" + "="*50)
print("  8.1: Sampling dan Filtering Data (Versi Ultra-Fast)")
print("="*50)

# --- KOMPROMISI KECEPATAN: Gunakan sampel yang jauh lebih kecil ---
# Kita mengurangi dari 40% menjadi 10% untuk memastikan proses cepat.
sample_fraction = 0.10
X_sample, _, y_sample, _ = train_test_split(
    X_train_encoded, y_train_encoded, train_size=sample_fraction,
    stratify=y_train_encoded, random_state=42
)

X_sample = pd.DataFrame(X_sample, columns=X_train_encoded.columns).reset_index(drop=True)
y_sample = pd.Series(y_sample).reset_index(drop=True)

print(f"Menggunakan sampel data {sample_fraction*100}% ({X_sample.shape[0]} baris) untuk optimasi.")

min_samples_needed = 5
class_counts_sample = Counter(y_sample)
valid_classes = [cls for cls, count in class_counts_sample.items() if count >= min_samples_needed]

print(f"\nKelas yang akan dipertahankan (dengan >= {min_samples_needed} sampel):")
for cls_num in valid_classes:
    class_name = label_encoder.inverse_transform([cls_num])[0]
    print(f"  - {class_name} (label {cls_num}): {class_counts_sample[cls_num]} sampel")

mask = y_sample.isin(valid_classes)
X_sample_filtered = X_sample[mask]
y_sample_filtered = y_sample[mask]

print(f"\nShape data sampel setelah filtering: {X_sample_filtered.shape}")


# --- 8.2: Terapkan SMOTE dan Mapping Ulang Label ---
print("\n" + "="*50)
print("  8.2: Terapkan SMOTE dan Mapping Ulang Label")
print("="*50)

# Buat pemeta dari label lama ke label baru yang berurutan
unique_filtered_labels = sorted(y_sample_filtered.unique())
label_map = {old_label: new_label for new_label, old_label in enumerate(unique_filtered_labels)}
print("Pemetaan label lama ke label baru (berurutan):")
print(label_map)

# Terapkan pemetaan ke y
y_sample_mapped = y_sample_filtered.map(label_map)

class_counts_filtered = Counter(y_sample_mapped)
min_class_count_filtered = min(class_counts_filtered.values())
safe_n_neighbors = min_class_count_filtered - 1
if safe_n_neighbors < 1: safe_n_neighbors = 1

print(f"\nJumlah sampel di kelas minoritas terkecil (setelah filter): {min_class_count_filtered}")
print(f"Menggunakan n_neighbors yang aman: {safe_n_neighbors}")

smote = SMOTE(random_state=42, k_neighbors=safe_n_neighbors)
X_sample_resampled, y_sample_resampled = smote.fit_resample(X_sample_filtered, y_sample_mapped)

print(f"Shape data sampel setelah SMOTE: {X_sample_resampled.shape}")
print(f"Jumlah kelas unik di data resampled (setelah mapping): {len(np.unique(y_sample_resampled))}")

# --- 8.3: Opsi "Nuklir" - Gunakan Model Default ---
print("\n" + "="*50)
print("  8.3: Menggunakan Model Default (Opsi Tercepat)")
print("="*50)

# Gunakan model XGBoost dengan parameter default

best_model = xgb.XGBClassifier(
    objective='multi:softmax',
    num_class=len(label_encoder.classes_),
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42,
    n_jobs=-1
)

print("Melatih model default pada seluruh data training...")
start_time = time.time()
best_model.fit(X_train_encoded, y_train_encoded)
end_time = time.time()
print(f"Pelatihan selesai dalam {end_time - start_time:.2f} detik.")

# --- 8.4: Evaluasi Model Default ---
print("\n" + "="*50)
print("  8.4: Evaluasi Model Default")
print("="*50)

y_pred_best_encoded = best_model.predict(X_test_filtered)
y_pred_best = label_encoder.inverse_transform(y_pred_best_encoded)

accuracy_best = accuracy_score(y_test_filtered, y_pred_best)
print(f"Akurasi Model Default: {accuracy_best:.4f}")

print("\n--- Laporan Klasifikasi Model Default ---")
print(classification_report(y_test_filtered, y_pred_best))

# ==============================================================================
# LANGKAH 8: PELATIHAN MODEL XGBOOST (VERSI CEPAT TANPA TUNING)
# ==============================================================================

print("\n" + "="*50)
print("      PELATIHAN MODEL XGBOOST (VERSI CEPAT)")
print("="*50)

# Inisialisasi model XGBoost dengan parameter default
# Parameter default seringkali sudah cukup baik untuk banyak kasus
model = xgb.XGBClassifier(
    objective='multi:softmax',
    num_class=len(label_encoder.classes_),
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42,
    n_jobs=-1 # Gunakan semua CPU core untuk mempercepat
)

# Melatih model menggunakan SELURUH data training yang sudah diproses
# Kita menggunakan X_train_encoded dan y_train_encoded (tanpa SMOTE, tanpa sampling)
print("Memulai pelatihan model pada seluruh data training...")
import time
start_time = time.time()
model.fit(X_train_encoded, y_train_encoded)
end_time = time.time()
print(f"Model berhasil dilatih dalam {end_time - start_time:.2f} detik.")

# ==============================================================================
# LANGKAH 9: EVALUASI MODEL YANG MENDALAM
# ==============================================================================

print("\n" + "="*50)
print("          EVALUASI MODEL YANG MENDALAM")
print("="*50)

# --- 9.1: Evaluasi Kuantitatif ---
# Buat prediksi pada data testing yang sudah disaring (X_test_filtered)
y_pred_encoded = model.predict(X_test_filtered)

# Kembalikan prediksi dari bentuk numerik ke label asli (teks)
y_pred = label_encoder.inverse_transform(y_pred_encoded)

# Hitung akurasi
accuracy = accuracy_score(y_test_filtered, y_pred)
print(f"Akurasi Model: {accuracy:.4f}")

# Tampilkan laporan klasifikasi yang detail
print("\n--- Laporan Klasifikasi Detail ---")
print(classification_report(y_test_filtered, y_pred))


# --- 9.2: Evaluasi Kualitatif (Visualisasi) ---
# --- 9.2.1: Confusion Matrix ---
print("\nMembuat Confusion Matrix...")
cm = confusion_matrix(y_test_filtered, y_pred)

plt.figure(figsize=(20, 18))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix', fontsize=20)
plt.ylabel('Label Asli (Actual)', fontsize=15)
plt.xlabel('Label Prediksi (Predicted)', fontsize=15)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.show()


# --- 9.2.2: Feature Importance ---
print("\nMembuat plot Feature Importance...")
importances = model.feature_importances_
features = X_train_encoded.columns

feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).head(20)

plt.figure(figsize=(12, 10))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')
plt.title('20 Fitur Paling Penting Menurut Model XGBoost', fontsize=20)
plt.xlabel('Tingkat Kepentingan (Importance Score)', fontsize=15)
plt.ylabel('Fitur', fontsize=15)
plt.show()

# ==============================================================================
# LANGKAH 11: PENEBARAN MODEL (DEPLOYMENT)
# ==============================================================================

print("\n" + "="*50)
print("          PENEBARAN MODEL (DEPLOYMENT)")
print("="*50)

# --- 11.1: Menyimpan (Serialisasi) Model ---
# Kita akan menggunakan library joblib, yang efisien untuk objek besar seperti model ML
from joblib import dump, load

# Simpan model ke dalam file 'xgboost_cyber_attack_model.joblib'
model_filename = 'xgboost_cyber_attack_model.joblib'
dump(model, model_filename)
dump(label_encoder, 'label_encoder.joblib') # Juga simpan label encoder-nya!

print(f"Model dan Label Encoder berhasil disimpan ke dalam file '{model_filename}' dan 'label_encoder.joblib'")

# --- 11.2: Memuat Prediksi pada Data Baru (Simulasi) ---
# Sekarang, bayangkan kita mendapatkan data koneksi baru yang belum pernah dilihat.
# Kita akan mensimulasinya dengan mengambil satu baris dari data testing.

# Ambil satu baris acak dari data testing asli (sebelum di-encode)
new_data_raw = test_df.sample(1, random_state=42)
print("\n--- Simulasi Data Baru ---")
display(new_data_raw)

# Pisahkan fitur dari label
new_data_features = new_data_raw.drop('labels', axis=1)
true_label = new_data_raw['labels'].iloc[0]

# --- 11.3: Memuat Model dan Memprediksi ---
# Muat kembali model dan label encoder yang sudah kita simpan
loaded_model = load(model_filename)
loaded_encoder = load('label_encoder.joblib')
print("\nModel dan Label Encoder berhasil dimuat kembali.")

# Penting: Data baru harus diproses dengan cara yang SAMA persis seperti data training
# 1. One-Hot Encoding
new_data_encoded = pd.get_dummies(new_data_features).reindex(columns=X_train_encoded.columns, fill_value=0)

# 2. Prediksi
prediction_encoded = loaded_model.predict(new_data_encoded)
prediction_label = loaded_encoder.inverse_transform(prediction_encoded)

print(f"\n--- Hasil Prediksi ---")
print(f"Label Asli Data Baru: {true_label}")
print(f"Label Prediksi Model: {prediction_label[0]}")

if true_label == prediction_label[0]:
    print("Hasil: Prediksi BENAR!")
else:
    print("Hasil: Prediksi SALAH.")

print("\nProses deployment simulasi selesai. Model siap digunakan untuk memprediksi data baru.")

# ==============================================================================
# LANGKAH 12: KESIMPULAN AKHIR
# ==============================================================================

print("\n" + "="*60)
print("                    KESIMPULAN AKHIR PROYEK")
print("="*60)

print(f"\n1. Model Akhir: Kita telah berhasil membangun model XGBoost untuk mendeteksi serangan siber.")
print(f"   Model ini mencapai akurasi sebesar {accuracy:.4f} pada data testing.")

print("\n2. Proses yang Ditempuh:")
print("   - Memuat dan membersihkan data.")
print("   - Melakukan EDA untuk memahami karakteristik data, terutama ketidakseimbangan kelas.")
print("   - Memproses fitur kategorikal dengan One-Hot Encoding secara konsisten.")
print("   - Melatih model XGBoost pada seluruh data training.")
print("   - Mengevaluasi model secara mendalam dengan metrik dan visualisasi.")
print("   - Menyimpan model untuk keperluan deployment di masa depan.")

print("\n3. Wawasan Utama:")
print("   - Fitur seperti 'src_bytes', 'dst_bytes', dan 'count' adalah indikator terkuat untuk deteksi anomali.")
print("   - Model sangat baik dalam mendeteksi kelas mayoritas ('normal', 'neptune').")
print("   - Tantangan terbesar adalah mendeteksi kelas serangan minoritas, yang menjadi fokus utama dalam perbaikan model.")

print("\n4. Langkah Selanjutnya (Jika Ingin Meningkatkan Model):")
print("   - Terapkan Hyperparameter Tuning dengan sumber daya yang memadai.")
print("   - Gunakan teknik penanganan ketidakseimbangan kelas yang lebih canggih (SMOTE, class_weight).")
print("   - Coba algoritma lain seperti LightGBM atau model deep learning untuk perbandingan.")

print("\n" + "="*60)
print("          PROYEK DETEKSI SERANGAN SIBER SELESAI")
print("="*60)